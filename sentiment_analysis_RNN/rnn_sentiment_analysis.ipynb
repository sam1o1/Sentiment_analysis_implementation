{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhttZKywAIJs"
      },
      "source": [
        "# Sentiment Analysis \n",
        "\n",
        "_Natural Langauge Processing Nanodegree Program_\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GWwlSBHAIKA"
      },
      "source": [
        "## Step 5: Switching gears - RNNs\n",
        "\n",
        "We just saw how the task of sentiment analysis can be solved via a traditional machine learning approach: BoW + a nonlinear classifier. We now switch gears and use Recurrent Neural Networks, and in particular LSTMs, to perform sentiment analysis in Keras. Conveniently, Keras has a built-in [IMDb movie reviews dataset](https://keras.io/datasets/#imdb-movie-reviews-sentiment-classification) that we can use, with the same vocabulary size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8n8RhxSAIKD",
        "outputId": "0e59474f-9418-4202-f9fa-a20c9d762997"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n",
            "17473536/17464789 [==============================] - 0s 0us/step\n",
            "Loaded dataset with 25000 training samples, 25000 test samples\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.datasets import imdb  # import the built-in imdb dataset in Keras\n",
        "\n",
        "# Set the vocabulary size\n",
        "vocabulary_size = 8000\n",
        "\n",
        "# Load in training and test data (note the difference in convention compared to scikit-learn)\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=vocabulary_size)\n",
        "print(\"Loaded dataset with {} training samples, {} test samples\".format(len(X_train), len(X_test)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RdHMjOhhAIKK",
        "outputId": "81b251ae-d118-4caf-d19b-055985ebd3bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Review ---\n",
            "[1, 4, 2, 716, 4, 65, 7, 4, 689, 4367, 6308, 2343, 4804, 2, 2, 5270, 2, 2315, 2, 2, 2, 2, 4, 2, 628, 7685, 37, 9, 150, 4, 2, 4069, 11, 2909, 4, 2, 847, 313, 6, 176, 2, 9, 6202, 138, 9, 4434, 19, 4, 96, 183, 26, 4, 192, 15, 27, 5842, 799, 7101, 2, 588, 84, 11, 4, 3231, 152, 339, 5206, 42, 4869, 2, 6293, 345, 4804, 2, 142, 43, 218, 208, 54, 29, 853, 659, 46, 4, 882, 183, 80, 115, 30, 4, 172, 174, 10, 10, 1001, 398, 1001, 1055, 526, 34, 3717, 2, 5262, 2, 17, 4, 6706, 1094, 871, 64, 85, 22, 2030, 1109, 38, 230, 9, 4, 4324, 2, 251, 5056, 1034, 195, 301, 14, 16, 31, 7, 4, 2, 8, 783, 2, 33, 4, 2945, 103, 465, 2, 42, 845, 45, 446, 11, 1895, 19, 184, 76, 32, 4, 5310, 207, 110, 13, 197, 4, 2, 16, 601, 964, 2152, 595, 13, 258, 4, 1730, 66, 338, 55, 5312, 4, 550, 728, 65, 1196, 8, 1839, 61, 1546, 42, 2, 61, 602, 120, 45, 7304, 6, 320, 786, 99, 196, 2, 786, 5936, 4, 225, 4, 373, 1009, 33, 4, 130, 63, 69, 72, 1104, 46, 1292, 225, 14, 66, 194, 2, 1703, 56, 8, 803, 1004, 6, 2, 155, 11, 4, 2, 3231, 45, 853, 2029, 8, 30, 6, 117, 430, 19, 6, 2, 9, 15, 66, 424, 8, 2337, 178, 9, 15, 66, 424, 8, 1465, 178, 9, 15, 66, 142, 15, 9, 424, 8, 28, 178, 662, 44, 12, 17, 4, 130, 898, 1686, 9, 6, 5623, 267, 185, 430, 4, 118, 2, 277, 15, 4, 1188, 100, 216, 56, 19, 4, 357, 114, 2, 367, 45, 115, 93, 788, 121, 4, 2, 79, 32, 68, 278, 39, 8, 818, 162, 4165, 237, 600, 7, 98, 306, 8, 157, 549, 628, 11, 6, 2, 13, 824, 15, 4104, 76, 42, 138, 36, 774, 77, 1059, 159, 150, 4, 229, 497, 8, 1493, 11, 175, 251, 453, 19, 2, 189, 12, 43, 127, 6, 394, 292, 7, 2, 4, 107, 8, 4, 2826, 15, 1082, 1251, 9, 906, 42, 1134, 6, 66, 78, 22, 15, 13, 244, 2519, 8, 135, 233, 52, 44, 10, 10, 466, 112, 398, 526, 34, 4, 1572, 4413, 6706, 1094, 225, 57, 599, 133, 225, 6, 227, 7, 541, 4323, 6, 171, 139, 7, 539, 2, 56, 11, 6, 3231, 21, 164, 25, 426, 81, 33, 344, 624, 19, 6, 4617, 7, 2, 2, 6, 5802, 4, 22, 9, 1082, 629, 237, 45, 188, 6, 55, 655, 707, 6371, 956, 225, 1456, 841, 42, 1310, 225, 6, 2493, 1467, 7722, 2828, 21, 4, 2, 9, 364, 23, 4, 2228, 2407, 225, 24, 76, 133, 18, 4, 189, 2293, 10, 10, 814, 11, 2, 11, 2642, 14, 47, 15, 682, 364, 352, 168, 44, 12, 45, 24, 913, 93, 21, 247, 2441, 4, 116, 34, 35, 1859, 8, 72, 177, 9, 164, 8, 901, 344, 44, 13, 191, 135, 13, 126, 421, 233, 18, 259, 10, 10, 4, 2, 6847, 4, 2, 3074, 7, 112, 199, 753, 357, 39, 63, 12, 115, 2, 763, 8, 15, 35, 3282, 1523, 65, 57, 599, 6, 1916, 277, 1730, 37, 25, 92, 202, 6, 2, 44, 25, 28, 6, 22, 15, 122, 24, 4171, 72, 33, 32]\n",
            "--- Label ---\n",
            "0\n"
          ]
        }
      ],
      "source": [
        "# Inspect a sample review and its label\n",
        "print(\"--- Review ---\")\n",
        "print(X_train[7])\n",
        "print(\"--- Label ---\")\n",
        "print(y_train[7])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIH8D6hRAIKM"
      },
      "source": [
        "Notice that the label is an integer (0 for negative, 1 for positive), and the review itself is stored as a sequence of integers. These are word IDs that have been preassigned to individual words. To map them back to the original words, you can use the dictionary returned by `imdb.get_word_index()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_raLg-MAIKN",
        "outputId": "bcafcedc-d9af-4a36-aa84-708c48808cfc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 0s 0us/step\n",
            "1654784/1641221 [==============================] - 0s 0us/step\n",
            "--- Review (with words) ---\n",
            "['the', 'of', 'and', 'local', 'of', 'their', 'br', 'of', 'attention', 'widow', 'alexandre', 'captures', 'parties', 'and', 'and', 'lands', 'and', 'excitement', 'and', 'and', 'and', 'and', 'of', 'and', 'english', 'crushed', 'like', 'it', 'years', 'of', 'and', 'unintentional', 'this', 'hitchcock', 'of', 'and', 'learn', 'everyone', 'is', 'quite', 'and', 'it', 'arab', 'such', 'it', 'bonus', 'film', 'of', 'too', 'seems', 'he', 'of', 'enough', 'for', 'be', 'blatantly', 'editing', 'fishburne', 'and', 'please', 'great', 'this', 'of', 'shoots', 'thing', '3', 'rooney', \"it's\", 'mentioning', 'and', 'carmen', 'given', 'parties', 'and', 'back', 'out', 'interesting', 'times', 'no', 'all', 'average', 'talking', 'some', 'of', 'nor', 'seems', 'into', 'best', 'at', 'of', 'every', 'cast', 'i', 'i', 'inside', 'keep', 'inside', 'large', 'viewer', 'who', 'obscure', 'and', 'sopranos', 'and', 'movie', 'of', 'routines', 'entirely', \"you've\", 'see', 'because', 'you', 'deals', 'successful', 'her', 'anything', 'it', 'of', 'dedicated', 'and', 'hard', 'assassin', 'further', \"that's\", 'takes', 'as', 'with', 'by', 'br', 'of', 'and', 'in', 'minute', 'and', 'they', 'of', 'westerns', 'watch', 'seemed', 'and', \"it's\", 'lee', 'if', 'oh', 'this', 'japan', 'film', 'around', 'get', 'an', 'of', 'sunny', 'always', 'life', 'was', 'between', 'of', 'and', 'with', 'group', 'rate', 'code', \"film's\", 'was', 'although', 'of', 'arts', 'had', 'death', 'time', 'stark', 'of', 'anyway', 'romantic', 'their', 'won', 'in', 'kevin', 'only', 'flying', \"it's\", 'and', 'only', 'cut', 'show', 'if', 'bauer', 'is', 'star', 'stay', 'movies', 'both', 'and', 'stay', 'damaged', 'of', 'music', 'of', 'tell', 'missing', 'they', 'of', 'here', 'really', 'me', 'we', 'value', 'some', 'silent', 'music', 'as', 'had', 'thought', 'and', 'realized', 'she', 'in', 'sorry', 'reasons', 'is', 'and', '10', 'this', 'of', 'and', 'shoots', 'if', 'average', 'remembered', 'in', 'at', 'is', 'over', 'worse', 'film', 'is', 'and', 'it', 'for', 'had', 'absolutely', 'in', 'naive', 'want', 'it', 'for', 'had', 'absolutely', 'in', 'j', 'want', 'it', 'for', 'had', 'back', 'for', 'it', 'absolutely', 'in', 'one', 'want', 'shots', 'has', 'that', 'movie', 'of', 'here', 'write', 'whatsoever', 'it', 'is', 'macho', 'set', 'got', 'worse', 'of', 'where', 'and', 'once', 'for', 'of', 'accent', 'after', 'saw', 'she', 'film', 'of', 'rest', 'little', 'and', 'camera', 'if', 'best', 'way', 'elements', 'know', 'of', 'and', 'also', 'an', 'were', 'sense', 'or', 'in', 'realistic', 'actually', 'satan', \"he's\", 'score', 'br', 'any', 'himself', 'in', 'another', 'type', 'english', 'this', 'is', 'and', 'was', 'tom', 'for', 'dating', 'get', \"it's\", 'such', 'from', 'fantastic', 'will', 'pace', 'new', 'years', 'of', 'guy', 'game', 'in', 'murders', 'this', 'us', 'hard', 'lives', 'film', 'and', 'fact', 'that', 'out', 'end', 'is', 'getting', 'together', 'br', 'and', 'of', 'seen', 'in', 'of', 'jail', 'for', 'sees', 'utterly', 'it', 'meet', \"it's\", 'depth', 'is', 'had', 'do', 'you', 'for', 'was', 'rather', 'convince', 'in', 'why', 'last', 'very', 'has', 'i', 'i', 'throughout', 'never', 'keep', 'viewer', 'who', 'of', 'becoming', 'switch', 'routines', 'entirely', 'music', 'even', 'interest', 'scene', 'music', 'is', 'far', 'br', 'voice', 'riveting', 'is', 'again', 'something', 'br', 'decent', 'and', 'she', 'this', 'is', 'shoots', 'not', 'director', 'have', 'against', 'people', 'they', 'line', 'cinematography', 'film', 'is', 'couples', 'br', 'and', 'and', 'is', 'daniels', 'of', 'you', 'it', 'sees', 'hero', \"he's\", 'if', \"can't\", 'is', 'time', 'husband', 'silly', 'hindi', 'result', 'music', 'image', 'sequences', \"it's\", 'chase', 'music', 'is', 'veteran', 'include', 'participate', 'freeman', 'not', 'of', 'and', 'it', 'along', 'are', 'of', 'hearing', 'cutting', 'music', 'his', 'get', 'scene', 'but', 'of', 'fact', 'correct', 'i', 'i', 'means', 'this', 'and', 'this', 'blockbuster', 'as', 'there', 'for', 'disappointed', 'along', 'wrong', 'few', 'has', 'that', 'if', 'his', 'weird', 'way', 'not', 'girl', 'display', 'of', 'love', 'who', 'so', 'friendship', 'in', 'we', 'down', 'it', 'director', 'in', 'situation', 'line', 'has', 'was', 'big', 'why', 'was', 'your', 'supposed', 'last', 'but', 'especially', 'i', 'i', 'of', 'and', 'stream', 'of', 'and', 'internet', 'br', 'never', 'give', 'theme', 'rest', 'or', 'really', 'that', 'best', 'and', 'release', 'in', 'for', 'so', 'multi', 'random', 'their', 'even', 'interest', 'is', 'judge', 'once', 'arts', 'like', 'have', 'then', 'own', 'is', 'and', 'has', 'have', 'one', 'is', 'you', 'for', 'off', 'his', 'dutch', 'we', 'they', 'an']\n",
            "--- Label ---\n",
            "0\n"
          ]
        }
      ],
      "source": [
        "# Map word IDs back to words\n",
        "word2id = imdb.get_word_index()\n",
        "id2word = {i: word for word, i in word2id.items()}\n",
        "print(\"--- Review (with words) ---\")\n",
        "print([id2word.get(i, \" \") for i in X_train[7]])\n",
        "print(\"--- Label ---\")\n",
        "print(y_train[7])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_len=None\n",
        "min_len=None \n",
        "for i in range(len(X_train)):\n",
        "  if (max_len == None) or (len(X_train[i]) > max_len):\n",
        "    max_len=len(X_train[i])\n",
        "  elif (min_len == None) or (len(X_train[i]) < min_len):\n",
        "    min_len=len(X_train[i])\n",
        "print(max_len)\n",
        "print(min_len)\n",
        "  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1E2bbT4CqZT",
        "outputId": "c8b82554-d142-47da-a64a-2708ef039f46"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2494\n",
            "11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVZp3_2gAIKP"
      },
      "source": [
        "Unlike our Bag-of-Words approach, where we simply summarized the counts of each word in a document, this representation essentially retains the entire sequence of words (minus punctuation, stopwords, etc.). This is critical for RNNs to function. But it also means that now the features can be of different lengths!\n",
        "\n",
        "#### Question: Variable length reviews\n",
        "\n",
        "What is the maximum review length (in terms of number of words) in the training set? What is the minimum?\n",
        "\n",
        "#### Answer:\n",
        "The maximum number of words is `2494`\n",
        "the minimum number of words is `11`\n",
        "\n",
        "...\n",
        "\n",
        "\n",
        "### TODO: Pad sequences\n",
        "\n",
        "In order to feed this data into your RNN, all input documents must have the same length. Let's limit the maximum review length to `max_words` by truncating longer reviews and padding shorter reviews with a null value (0). You can accomplish this easily using the [`pad_sequences()`](https://keras.io/preprocessing/sequence/#pad_sequences) function in Keras. For now, set `max_words` to `2494`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Pi7G6VuFAIKS"
      },
      "outputs": [],
      "source": [
        " \n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Set the maximum number of words per document (for both training and testing)\n",
        "max_words = 500\n",
        "\n",
        "# TODO: Pad sequences in X_train and X_test\n",
        "X_train=pad_sequences(X_train,maxlen=max_words,padding='post',truncating='post')\n",
        "X_test=pad_sequences(X_test,maxlen=max_words,padding='post',truncating='post')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ta6ZJZPoAIKV"
      },
      "source": [
        "### TODO: Design an RNN model for sentiment analysis\n",
        "\n",
        "Build your model architecture in the code cell below. We have imported some layers from Keras that you might need but feel free to use any other layers / transformations you like.\n",
        "\n",
        "Remember that your input is a sequence of words (technically, integer word IDs) of maximum length = `max_words`, and your output is a binary sentiment label (0 or 1)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout"
      ],
      "metadata": {
        "id": "wCuPIWEbHAtb"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pidWqbH9AIKY",
        "outputId": "8ed715d6-70dd-47be-d58e-468d5c6bfd00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 500, 100)          800000    \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 200)               240800    \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 201       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,041,001\n",
            "Trainable params: 1,041,001\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# TODO: Design your model\n",
        "\n",
        "model =Sequential([\n",
        "    Embedding(vocabulary_size, 100, input_length=max_words),\n",
        "    LSTM(200),\n",
        "    Dense(1,activation='sigmoid')\n",
        "])\n",
        "print(model.summary())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0qHhDWCAIKa"
      },
      "source": [
        "#### Question: Architecture and parameters\n",
        "\n",
        "Briefly describe your neural net architecture. How many model parameters does it have that need to be trained?\n",
        "\n",
        "#### Answer:\n",
        "The architecture i used is as follows:\n",
        "\n",
        "\n",
        "*   Embedding Layer to covert the input sequence to vectors.\n",
        "*   Averagepooling1D instead of the flatten.\n",
        "*   LSTM Cell \n",
        "*   Two Dense layers. \n",
        "\n",
        "The number of trainable params is `435,457`\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "...\n",
        "\n",
        "### TODO: Train and evaluate your model\n",
        "\n",
        "Now you are ready to train your model. In Keras world, you first need to _compile_ your model by specifying the loss function and optimizer you want to use while training, as well as any evaluation metrics you'd like to measure. Specify the approprate parameters, including at least one metric `'accuracy'`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "collapsed": true,
        "id": "4cPW086sAIKb"
      },
      "outputs": [],
      "source": [
        "# TODO: Compile your model, specifying a loss function, optimizer, and metrics\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSlH1tiWAIKd"
      },
      "source": [
        "Once compiled, you can kick off the training process. There are two important training parameters that you have to specify - **batch size** and **number of training epochs**, which together with your model architecture determine the total training time.\n",
        "\n",
        "Training may take a while, so grab a cup of coffee, or better, go for a hike! If possible, consider using a GPU, as a single training run can take several hours on a CPU.\n",
        "\n",
        "> **Tip**: You can split off a small portion of the training set to be used for validation during training. This will help monitor the training process and identify potential overfitting. You can supply a validation set to `model.fit()` using its `validation_data` parameter, or just specify `validation_split` - a fraction of the training data for Keras to set aside for this purpose (typically 5-10%). Validation metrics are evaluated once at the end of each epoch."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2eKIdq61Lle6",
        "outputId": "32eafca4-f6ab-4a30-f304-b69dde7b2543"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   1,   14,   22, ...,    0,    0,    0],\n",
              "       [   1,  194, 1153, ...,    0,    0,    0],\n",
              "       [   1,   14,   47, ...,    0,    0,    0],\n",
              "       [   1,    4,    2, ...,    0,    0,    0],\n",
              "       [   1,  249, 1323, ...,    0,    0,    0]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Validation_size=int(0.1*len(X_train))"
      ],
      "metadata": {
        "id": "YI2YfsHOLNJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-frcPHqAIKe",
        "outputId": "c803df0f-ae62-478d-885c-e78e0eba321c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the training size---data 24872,--labels 24872\n",
            "the validation size---data 128,--labels 128\n"
          ]
        }
      ],
      "source": [
        "# TODO: Specify training parameters: batch size and number of epochs\n",
        "batch_size = 128\n",
        "num_epochs = 5\n",
        "X_vald=X_train[:batch_size]\n",
        "X_train=X_train[batch_size:]\n",
        "y_vald=y_train[:batch_size]\n",
        "y_train=y_train[batch_size:]\n",
        "print('the training size---data {},--labels {}'.format(len(X_train),len(y_train)))\n",
        "print('the validation size---data {},--labels {}'.format(len(X_vald),len(y_vald)))\n",
        "# TODO: Train your model\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train, y_train,\n",
        "          validation_data=(X_vald, y_vald),\n",
        "          batch_size=batch_size, epochs=num_epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2eox7HpWO6Yy",
        "outputId": "302b53e0-75e7-4433-bc9b-c9a2e143ac8f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "195/195 [==============================] - 36s 155ms/step - loss: 0.6931 - accuracy: 0.5087 - val_loss: 0.6948 - val_accuracy: 0.4219\n",
            "Epoch 2/5\n",
            "195/195 [==============================] - 30s 152ms/step - loss: 0.7010 - accuracy: 0.5111 - val_loss: 0.6990 - val_accuracy: 0.4375\n",
            "Epoch 3/5\n",
            "195/195 [==============================] - 30s 152ms/step - loss: 0.6746 - accuracy: 0.5294 - val_loss: 0.7102 - val_accuracy: 0.5703\n",
            "Epoch 4/5\n",
            "195/195 [==============================] - 30s 153ms/step - loss: 0.6504 - accuracy: 0.5371 - val_loss: 0.7772 - val_accuracy: 0.4297\n",
            "Epoch 5/5\n",
            "195/195 [==============================] - 30s 152ms/step - loss: 0.6437 - accuracy: 0.5432 - val_loss: 0.7628 - val_accuracy: 0.4375\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f4ded449810>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Notes \n",
        "After training the first model with the max_len as the maximum number of woords in the training data,the batch size is 128 and using average pooling and two dense layers, The results came out to be very poor on both the training and validation data. I folloed the architechture in this [paper](https://ieeexplore.ieee.org/abstract/document/9303573) to enhance the the model's performance. "
      ],
      "metadata": {
        "id": "Kh1KjayWTyrV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "fvsJeMiEAIKg"
      },
      "outputs": [],
      "source": [
        "# Save your model, so that you can quickly load it in future (and perhaps resume training)\n",
        "model_file = \"rnn_model.h5\"  # HDF5 file\n",
        "model.save( model_file)\n",
        "\n",
        "# Later you can load it using keras.models.load_model()\n",
        "#from keras.models import load_model\n",
        "#model = load_model(os.path.join(cache_dir, model_file))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YiR8d54AIKh"
      },
      "source": [
        "Once you have trained your model, it's time to see how well it performs on unseen test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "FvWSG120AIKi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51c72a90-1991-4779-de48-3a9cbc807ffa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 0.5019199848175049\n"
          ]
        }
      ],
      "source": [
        "# Evaluate your model on the test set\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)  # returns loss and other metrics specified in model.compile()\n",
        "print(\"Test accuracy:\", scores[1])  # scores[1] should correspond to accuracy if you passed in metrics=['accuracy']"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "rnn_sentiment_analysis.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}